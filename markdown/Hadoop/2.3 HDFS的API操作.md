# 环境准备

希望能在Windows里编辑代码，远程连接hadoop进行调试或执行



## **windows环境变量**

将hadoop依赖放置全英文目录

在windows环境变量全局里添加HADOOP_HOME 并指向上面的目录

在path里添加 %HADOOP_HOME%\bin



## **IDEA创建工程**

在 IDEA 中创建一个 Maven 工程 HdfsClientDemo

导入相应的依赖坐标+日志添加

```html
<dependencies>
 <dependency>
 <groupId>org.apache.hadoop</groupId>
 <artifactId>hadoop-client</artifactId>
 <version>3.1.3</version>
 </dependency>
 <dependency>
 <groupId>junit</groupId>
 <artifactId>junit</artifactId>
 <version>4.12</version>
 </dependency>
 <dependency>
 <groupId>org.slf4j</groupId>
 <artifactId>slf4j-log4j12</artifactId>
 <version>1.7.30</version>
 </dependency>
</dependencies>
```



在项目的 src/main/resources 目录下，新建一个文件，命名为“log4j.properties”，在文件 中填入

```
log4j.rootLogger=INFO, stdout 
log4j.appender.stdout=org.apache.log4j.ConsoleAppender 
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout 
log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n 
log4j.appender.logfile=org.apache.log4j.FileAppender 
log4j.appender.logfile.File=target/spring.log 
log4j.appender.logfile.layout=org.apache.log4j.PatternLayout 
log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
```



# 测试运行

创建包名：com.atguigu.hdfs

测试创建文件夹代码：

```JAva
package com.atguigu.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.junit.Test;
import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

/*
* 客户端执行常用套路
*   1.获取一个客户端对象
*   2.执行相关操作命令
*   3.关闭资源
* */

public class HdfsClient {
    @Test
    public void testmkdir() throws URISyntaxException, IOException, InterruptedException {
        // 连接集群nn地址
        URI uri = new URI("hdfs://192.168.10.102:8020");
        // 配置对象
        Configuration configuration = new Configuration();
        // 登录用户名
        String user = "atguigu";
        // 获取客户端对象（因为是抽象类所以不能new）
        FileSystem fs = FileSystem.get(uri, configuration, user);
        // 新建文件夹
        fs.mkdirs(new Path("/xiyou/huoguoshan"));
        // 关闭客户端
        fs.close();
    }
}

```



# 代码样例

以下的代码 fs是提前创建的对象

## 创建文件夹

```java
//创建文件夹
@Test
public void testmkdir() throws IOException, URISyntaxException, InterruptedException {
    fs.mkdirs(new Path("/xiyou/hgs"));
}
```



## 文件上传

```java
//文件上传
@Test
public void testPut() throws IOException, URISyntaxException, InterruptedException {
    //参数解读：参数一：表示删除原数据  参数二：如果已存在覆盖 参数三：原数据路径   参数四：目的地路径
    fs.copyFromLocalFile(true, true, new Path("D:/Hadoop/sgg/temp/sunwukong.txt"), new Path("hdfs://192.168.10.102/xiyou/huoguoshan"));
}
```



## 文件下载

```java
//文件下载
@Test
public void testGet() throws IOException, URISyntaxException, InterruptedException {
    //参数解读：参数一：表示删除原数据 参数二：HDFS数据路径   参数三：本地路径	参数四：useRawLocalFileSystem
    fs.copyToLocalFile(false, new Path("hdfs://192.168.10.102/xiyou/huoguoshan/sunwukong.txt"), new Path("D:/Hadoop/sgg/temp/outdoor.txt"), false);
}
```

如果第四个参数写true， 下载完了会多一个crc文件，这个是循环校验码

如果写false就没有。useRawLocalFileSystem指示是否使用RawLocalFileSystem作为本地文件系统。RawLocalFileSystem不是校验和，所以它不会在本地创建任何crc文件。



## 文件删除

```java
//文件删除
@Test
public void testRm() throws IOException, URISyntaxException, InterruptedException {
    //参数解读：参数一：删除的路径 参数二：是否递归删除
    fs.delete(new Path("hdfs://192.168.10.102/xiyou/huoguoshan/"), true);
}
```

如果删除的是一个文件夹，且非空，则一定要开递归删除

如果只是一个空文件夹 或一个文件，那没所谓



## 文件更名和移动

```java
//文件更名和移动(不保留源文件)
@Test
public void testMove() throws URISyntaxException, IOException, InterruptedException {
    //参数解读：参数一：源文件路径    参数二：目标文件路径
    fs.rename(new Path("hdfs://192.168.10.102/xiyou/huoguoshan"), new Path("hdfs://192.168.10.102/xiyou/huaguoshan.txt"));
}
```

也可以直接更名文件夹



## 看文件详细

```java
//列举文件信息
@Test
public void testFileDetail() throws IOException, URISyntaxException, InterruptedException {
    init();
    //参数解读：参数一：路径    参数二：递归
    RemoteIterator<LocatedFileStatus> listFiles = fs.listFiles(new Path("hdfs://192.168.10.102/xiyou"), true);
    //遍历迭代器
    while (listFiles.hasNext()) {
        LocatedFileStatus fileStatus = listFiles.next();
 
        System.out.println("=======" + fileStatus.getPath() + "=======");
        System.out.println("permission");
        System.out.println(fileStatus.getPermission());
        System.out.println("owner");
        System.out.println(fileStatus.getOwner());
        System.out.println("len");
        System.out.println(fileStatus.getLen());
        System.out.println("modificationTime");
        System.out.println(fileStatus.getModificationTime());
        System.out.println("replication");
        System.out.println(fileStatus.getReplication());
        System.out.println("bliocksize");
        System.out.println(fileStatus.getBlockSize());
        System.out.println("name");
        System.out.println(fileStatus.getPath().getName());
        System.out.println("blockLocations");
        BlockLocation[] blockLocations = fileStatus.getBlockLocations();
        System.out.println(Arrays.toString(blockLocations));
    }
}
```

输出：

```
=======hdfs://192.168.10.102/xiyou/hgs/1.txt=======
    permission
rw-r--r--
    owner
atguigu
    len
9
    modificationTime
1657207820138
    replication
3
    bliocksize
134217728
    name
1.txt
    blockLocations
[0,9,hadoop103,hadoop102,hadoop104]
```



## 判断是文件还是目录

```java
//文件夹和文件判断
@Test
public void testFile() throws URISyntaxException, IOException, InterruptedException {
    //参数解读：参数：文件路径
    FileStatus[] fileStatuses = fs.listStatus(new Path("hdfs://192.168.10.102/"));
    for (FileStatus fileStatus : fileStatuses) {
        if (fileStatus.isFile()) {
            System.out.println("文件："+fileStatus.getPath().getName());
        }else{
            System.out.println("目录："+fileStatus.getPath().getName());
        }
    }

}
```



# 配置项优先级

配置参数可以在很多地方调。参数每个地方写的不一样就会设置一个最高优先级的文件

参数优先级： hdfs-default.xml  <  hdfs-site.xml < 项目resources文件夹 < 代码 configuration.set()

