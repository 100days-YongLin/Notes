# ReduceJoin

Map 端的主要工作：为来自不同表或文件的 key/value 对，打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为 value，最后进行输出。 

Reduce 端的主要工作：在 Reduce 端以连接字段作为 key 的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录（在 Map 阶段已经打标志）分开，最后进行合并就 ok 了



# ReduceJoin案例

## 需求

<img src=".\picture\image-20220720151546786.png" alt="image-20220720151546786" style="zoom:50%;" />

<img src=".\picture\image-20220720151607134.png" alt="image-20220720151607134" style="zoom:50%;" />

## 需求分析

通过将关联条件作为 Map 输出的 key，将两表满足 Join 条件的数据并携带数据所来源 的文件信息，发往同一个 ReduceTask，在 Reduce 中进行数据的串联。

![image-20220720151707219](.\picture\image-20220720151707219.png)



## 代码实现

tableBean

```java
package com.atguigu.mapreduce.reduceJoin;

import org.apache.hadoop.io.Writable;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

public class TableBean implements Writable {
//    id pid amount pid pname
    private String id; //订单id
    private String pid; //商品id
    private int amount; //订单数量
    private String pname; //商品名称
    private String flag; //标记表名 order/pd

    public String getId() {
        return id;
    }

    public void setId(String id) {
        this.id = id;
    }

    public String getPid() {
        return pid;
    }

    public void setPid(String pid) {
        this.pid = pid;
    }

    public int getAmount() {
        return amount;
    }

    public void setAmount(int amount) {
        this.amount = amount;
    }

    public String getPname() {
        return pname;
    }

    public void setPname(String pname) {
        this.pname = pname;
    }

    public String getFlag() {
        return flag;
    }

    public void setFlag(String flag) {
        this.flag = flag;
    }

    //空参构造
    public TableBean() {

    }

    @Override
    public void write(DataOutput out) throws IOException {
        out.writeUTF(id);
        out.writeUTF(pid);
        out.writeInt(amount);
        out.writeUTF(pname);
        out.writeUTF(flag);
    }

    @Override
    public void readFields(DataInput in) throws IOException {
        this.id = in.readUTF();
        this.pid = in.readUTF();
        this.amount = in.readInt();
        this.pname = in.readUTF();
        this.flag = in.readUTF();
    }

    @Override
    public String toString() {
        return id + "\t" + pname + "\t" + amount;
    }
}
```



mapper

```java
package com.atguigu.mapreduce.reduceJoin;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;

import java.io.IOException;

public class TableMapper extends Mapper<LongWritable, Text, Text, TableBean> {
    //一个切片 创建一个MapTask, 所以可以在初始化里获取文件名
    private String fileName;
    private Text outK = new Text();
    private TableBean outV = new TableBean();
    @Override
    protected void setup(Mapper<LongWritable, Text, Text, TableBean>.Context context) throws IOException, InterruptedException {
        // 初始化 order pd 这个方法只调用一次

        //拿到切片
        FileSplit split = (FileSplit) context.getInputSplit();
        //获取切片文件名
        fileName = split.getPath().getName();
    }

    @Override
    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, TableBean>.Context context) throws IOException, InterruptedException {
        // 1 获取一行
        String line = value.toString();

        // 2 判断是哪张表
        if (fileName.contains("order")) { //处理的是订单表
            //切割
            String[] split = line.split("\t");

            //封装k v
            outK.set(split[1]); //pid

            outV.setId(split[0]);
            outV.setPid(split[1]);
            outV.setAmount(Integer.parseInt(split[2]));
            outV.setPname("");
            outV.setFlag("order");

        }else { //处理的是商品表
            //切割
            String[] split = line.split("\t");

            //封装
            outK.set(split[0]);

            outV.setId("");
            outV.setPid(split[0]);
            outV.setPname(split[1]);
            outV.setFlag("pd");
            outV.setAmount(0);
        }

        context.write(outK, outV);
    }
}
```



reducer

```java
package com.atguigu.mapreduce.reduceJoin;

import org.apache.commons.beanutils.BeanUtils;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;
import java.lang.reflect.InvocationTargetException;
import java.util.ArrayList;

public class TableReduce extends Reducer<Text, TableBean, TableBean, NullWritable> {

    @Override
    protected void reduce(Text key, Iterable<TableBean> values, Reducer<Text, TableBean, TableBean, NullWritable>.Context context) throws IOException, InterruptedException {
        //准备初始化集合
        ArrayList<TableBean> tableBeans = new ArrayList<>();
        TableBean pdBean = new TableBean();

        for (TableBean value : values) {
            if(value.getFlag().equals("order")){
                //tableBeans.add(value);不能这么写，Hadoop每次迭代的只是给地址，所以需要用一个新的对象装着值
                TableBean tempTableBean = new TableBean();
                try {
                    BeanUtils.copyProperties(tempTableBean, value); //赋值
                    tableBeans.add(tempTableBean);
                } catch (IllegalAccessException e) {
                    throw new RuntimeException(e);
                } catch (InvocationTargetException e) {
                    throw new RuntimeException(e);
                }

            }else{
                try {
                    BeanUtils.copyProperties(pdBean, value);
                } catch (IllegalAccessException e) {
                    throw new RuntimeException(e);
                } catch (InvocationTargetException e) {
                    throw new RuntimeException(e);
                }
            }
        }

        //循环遍历orderbeans 赋值pdname
        for (TableBean tableBean : tableBeans) {
            tableBean.setPname(pdBean.getPname());
            context.write(tableBean, NullWritable.get());
        }
    }
}
```



driver

```java
Job job = Job.getInstance(new Configuration());
job.setJarByClass(TableDriver.class);
job.setMapperClass(TableMapper.class);
job.setReducerClass(TableReduce.class);
job.setMapOutputKeyClass(Text.class);
job.setMapOutputValueClass(TableBean.class);
job.setOutputKeyClass(TableBean.class);
job.setOutputValueClass(NullWritable.class);
FileInputFormat.setInputPaths(job, new Path("D:\\Java\\HadoopCode\\MapreduceDemo\\MapReduceDemo\\src\\main\\java\\com\\atguigu\\mapreduce\\reduceJoin\\inputtable"));
FileOutputFormat.setOutputPath(job, new Path("D:\\Java\\HadoopCode\\MapreduceDemo\\MapReduceDemo\\src\\main\\java\\com\\atguigu\\mapreduce\\reduceJoin\\output"));
boolean b = job.waitForCompletion(true);
System.exit(b ? 0 : 1);
```



结果

```
1004	小米	4
1001	小米	1
1005	华为	5
1002	华为	2
1006	格力	6
1003	格力	3
```



## 缺点

缺点：这种方式中，合并的操作是在 Reduce 阶段完成，Reduce 端的处理压力太大，Map 节点的运算负载则很低，资源利用率不高，且在 Reduce 阶段极易产生数据倾斜。 

解决方案：Map 端实现数据合并。



# MapJoin

1）使用场景 

Map Join 适用于一张表十分小、一张表很大的场景。 

2）优点 思考：在 Reduce 端处理过多的表，非常容易产生数据倾斜。怎么办？

 在 Map 端缓存多张表，提前处理业务逻辑，这样增加 Map 端业务，减少 Reduce 端数 据的压力，尽可能的减少数据倾斜。 

3）具体办法：采用 DistributedCache

（1）在 Mapper 的 setup 阶段，将文件读取到缓存集合中。

（2）在 Driver 驱动类中加载缓存。

```
//缓存普通文件到 Task 运行节点。
job.addCacheFile(new URI("file:///e:/cache/pd.txt"));
//如果是集群运行,需要设置 HDFS 路径
job.addCacheFile(new URI("hdfs://hadoop102:8020/cache/pd.txt"));
```



# MapJoin案例

## 需求

<img src=".\picture\image-20220720173222966.png" alt="image-20220720173222966" style="zoom:50%;" />

<img src=".\picture\image-20220720173246006.png" alt="image-20220720173246006" style="zoom:50%;" />

<img src="C:\Users\mail1\Desktop\notes\markdown\Hadoop\picture\image-20220720173315213.png" alt="image-20220720173315213" style="zoom:50%;" />

MapJoin 适用于关联表中有小表的情形。

![image-20220720173400077](.\picture\image-20220720173400077.png)

## 代码实现

Drvier

```java
package com.atguigu.mapreduce.mapJoin;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

public class MapJoinDriver {
    public static void main(String[] args) throws IOException, URISyntaxException, InterruptedException, ClassNotFoundException {
        // 1 获取 job 信息
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);
        // 2 设置加载 jar 包路径
        job.setJarByClass(MapJoinDriver.class);
        // 3 关联 mapper
        job.setMapperClass(MapJoinMapper.class);
        // 4 设置 Map 输出 KV 类型
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(NullWritable.class);
        // 5 设置最终输出 KV 类型
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(NullWritable.class);
        // 加载缓存数据
        job.addCacheFile(new URI("file:///D:/Java/HadoopCode/MapreduceDemo/MapReduceDemo/src/main/java/com/atguigu/mapreduce/mapJoin/cache/pd.txt"));
        //        // Map 端 Join 的逻辑不需要 Reduce 阶段，设置 reduceTask 数量为 0
        job.setNumReduceTasks(0);
        // 6 设置输入输出路径
        FileInputFormat.setInputPaths(job, new Path("D:\\Java\\HadoopCode\\MapreduceDemo\\MapReduceDemo\\src\\main\\java\\com\\atguigu\\mapreduce\\mapJoin\\input"));
        FileOutputFormat.setOutputPath(job, new Path("D:\\Java\\HadoopCode\\MapreduceDemo\\MapReduceDemo\\src\\main\\java\\com\\atguigu\\mapreduce\\mapJoin\\output"));
        // 7 提交
        boolean b = job.waitForCompletion(true);
        System.exit(b ? 0 : 1);


    }
}
```



Mapper

```java
package com.atguigu.mapreduce.mapJoin;

import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.net.URI;
import java.util.HashMap;

public class MapJoinMapper extends Mapper<LongWritable, Text, Text, NullWritable> {
    private HashMap<String, String> pdMap = new HashMap<>();
    private Text outK = new Text();

    @Override
    protected void setup(Mapper<LongWritable, Text, Text, NullWritable>.Context context) throws IOException, InterruptedException {
        // 获取缓存文件，并把文件封装到集合 pd.txt
        URI[] cacheFiles = context.getCacheFiles();

        FileSystem fs = FileSystem.get(context.getConfiguration());
        FSDataInputStream fis = fs.open(new Path(cacheFiles[0]));

        //从流中读取数据
        BufferedReader reader = new BufferedReader(new InputStreamReader(fis, "UTF-8"));

        String line;
        line = reader.readLine();
        while(StringUtils.isNotEmpty(line)){
            // 切割
            String[] split = line.split("\t");

            //赋值
            pdMap.put(split[0], split[1]);

            line = reader.readLine();
        }
        IOUtils.closeStream(reader);
    }

    @Override
    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, NullWritable>.Context context) throws IOException, InterruptedException {
        // 处理order.txt
        String line = value.toString();
        String[] split = line.split("\t");

        // 获取pid
        String pname = pdMap.get(split[1]);

        //获取订单id 和 订单数量
        String pid = split[0];
        String pcount = split[2];

        //拼字符串
        outK.set(pid  + "\t" +  pname + "\t" + pcount);

        context.write(outK, NullWritable.get());
    }
}
```