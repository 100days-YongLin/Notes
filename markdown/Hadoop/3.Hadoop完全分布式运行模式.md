# 3.HADOOP完全分布式运行模式

## 分析

1.准备3台客户机（关闭防火墙、静态IP、主机名称）

2.安装jdk

3.配置环境变量

4.安装hadoop

5.配置环境变量

6.配置集群

7.单点启动

8.配置ssh

9.群起并测试集群



## 编写集群分发脚本-基于SCP

因为在之前的步骤中，在102机上完成了1-5步骤，接下来要进行其他步骤的编写

### **1.scp(secure copy)安全拷贝**

scp可以实现服务器与服务器之间的数据拷贝



**基本语法**

**scp	-r	$pdir/$fname	$user@$host:$pdir/$fname**

命令	递归	要拷贝的文件路径/名称	目的地用户@主机：目的地路径/名称



a.在 hadoop102 上，将 hadoop102 中/opt/module/jdk1.8.0_212 目录拷贝到 hadoop103 上【我给别人推】

[atguigu@hadoop102 ~]$ scp -r /opt/module/jdk1.8.0_212 atguigu@192.168.10.103:/opt/module



b.在 hadoop103 上，将 hadoop102 中/opt/module/hadoop-3.1.3 目录拷贝到 hadoop103 上。【我在别的服务器拉取数据】

[atguigu@hadoop103 ~]$ scp -r  atguigu@192.168.10.102:/opt/module/hadoop-3.1.3 /opt/module/



c.在 hadoop103 上操作，将 hadoop102 中/opt/module 目录下所有目录拷贝到 hadoop104 上。【从别的服务器到别的服务器】

[atguigu@hadoop103 opt]$ scp -r  atguigu@192.168.10.102:/opt/module/* atguigu@192.168.10.104:/opt/module



### 2.rsync 远程同步工具

rsync 主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。

rsync 和 scp 区别：用 rsync 做文件的复制要比 scp 的速度快，rsync 只对差异文件做更新。scp 是把所有文件都复制过去。



**基本语法**

**rsync	-av	$pdir/$fname	$user@$host:$pdir/$fname** 

命令	选项参数	要拷贝的文件路径/名称	目的地用户@主机:目的地路径/名称 选项参数说明

-a 归档拷贝 -v 显示复制过程



案例实操

a.删除 hadoop103 中/opt/module/hadoop-3.1.3/wcinput

[atguigu@hadoop103 hadoop-3.1.3]$ rm -rf wcinput/ 

b.同步 hadoop102 中的/opt/module/hadoop-3.1.3 到 hadoop103

[atguigu@hadoop102 module]$ rsync -av hadoop-3.1.3/  atguigu@hadoop103:/opt/module/hadoop-3.1.3/



### 3.xsync 分发脚本

为了比较方便的实现几台机子集群文件的分发，可以根据rsync写shell脚本完成此步骤

文件命名为xsync 并存放在/home/atguigu/bin里，因为这是全局变量目录



脚本如下

```shell
#!/bin/bash
#1. 判断参数个数
if [ $# -lt 1 ]
then
 echo Not Enough Arguement!
 exit;
fi
#2. 遍历集群所有机器
for host in 192.168.10.102 192.168.10.103 192.168.10.104
do
 echo ==================== $host ====================
 #3. 遍历所有目录，挨个发送
 for file in $@
 do
 #4. 判断文件是否存在
 if [ -e $file ]
 then
 #5. 获取父目录
 pdir=$(cd -P $(dirname $file); pwd)
 #6. 获取当前文件的名称
 fname=$(basename $file)
 ssh $host "mkdir -p $pdir"
 rsync -av $pdir/$fname $host:$pdir
 else
 echo $file does not exists!
 fi
 done
done
```



如果要传递一些root用户才能访问的目录，可以到了atguigui目录下 sudo ./bin/xsync 来操作



## SSH无密码登录

ssh可以登录别人的服务器并且执行命令

**基本语法**

ssh 另一台电脑的ip地址

exit # 退出



**运行逻辑**

1.被控制的服务器A，要先使用 **ssh-key-gen** 生成密钥对

会生成一个公钥A、私钥A

2.将公钥A发送给服务器B，授权给服务器B

3.A服务器 用ssh 访问服务器B，并且数据用私钥A加密

4.B服务器接收数据，并找有没有A的公钥A，并用公钥A加密

5.B服务器发送数据给A，采用公钥A加密 发送回A

5.仅有A服务器可以解包B加密的消息，用私钥A解密



**无密登录**

1.进入/home/atguigu/.ssh目录

2.**ssh-keygen -t rsa**

敲三个回车 目录下会生成

id_rsa（私钥）

id_rsa.pub（公钥）

3.拷贝到别的机子上

**ssh-copy-id 192.168.10.102** （不然自己对自己ssh也要密码）

**ssh-copy-id 192.168.10.103**

**ssh-copy-id 192.168.10.104**



然后再执行ssh就不需要密码了



注意：每一个用户都需要弄一个无密登录，我们只配了atguigu，需要的话把root也弄弄



## 集群配置

### 1.**集群部署规划**

NameNode 和 2NN(SecondaryNameNode)不能安装在同一机器上

ResourceManager也很耗费内存，不要和NameNode放在同一服务器上

所以

Hadoop102	 HDFS-**NameNode**、DateNode	YARN-NodeManager

Hadoop103	 HDFS-DateNode	YARN-**ResourceManager**、NodeManager

Hadoop104	 HDFS-**2NN**、DateNode	YARN-NodeManager



### 2.**配置文件说明**

配置文件分为：默认配置文件、自定义配置文件

用户想修改默认值时，才需要修改自定义配置文件，更改相应值



默认配置文件

[core-default.xml]	hadoop-common-3.1.3.jar/core-default.xml 

[hdfs-default.xml]	hadoop-hdfs-3.1.3.jar/hdfs-default.xml 

[yarn-default.xml]	hadoop-yarn-common-3.1.3.jar/yarn-default.xml 

[mapred-default.xml]	hadoop-mapreduce-client-core-3.1.3.jar/mapred-default.xml



自定义配置文件

**core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml** 四个配置文件存放在 $HADOOP_HOME/etc/hadoop 这个路径上，用户可以根据项目需求重新进行修改配置。



### 3.配置集群

核心配置文件

core-site.xml

```html
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
 <!-- 指定 NameNode 的地址 -->
 <property>
 <name>fs.defaultFS</name>
 <value>hdfs://hadoop102:8020</value>
 </property>
 <!-- 指定 hadoop 数据的存储目录 -->
 <property>
 <name>hadoop.tmp.dir</name>
 <value>/opt/module/hadoop-3.1.3/data</value>
 </property>
 <!-- 配置 HDFS 网页登录使用的静态用户为 atguigu -->
 <property>
 <name>hadoop.http.staticuser.user</name>
 <value>atguigu</value>
 </property>
</configuration>
```

HDFS 配置文件

hdfs-site.xml

```html
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<!-- nn web 端访问地址-->
<property>
 <name>dfs.namenode.http-address</name>
 <value>hadoop102:9870</value>
 </property>
<!-- 2nn web 端访问地址-->
 <property>
 <name>dfs.namenode.secondary.http-address</name>
 <value>hadoop104:9868</value>
 </property>
</configuration>
```

YARN 配置文件

配置 yarn-site.xml

```html
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
 <!-- 指定 MR 走 shuffle -->
 <property>
 <name>yarn.nodemanager.aux-services</name>
 <value>mapreduce_shuffle</value>
 </property>
 <!-- 指定 ResourceManager 的地址-->
 <property>
 <name>yarn.resourcemanager.hostname</name>
 <value>hadoop103</value>
 </property>
 <!-- 环境变量的继承 -->
 <property>
 <name>yarn.nodemanager.env-whitelist</name>
 
<value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CO
NF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAP
RED_HOME</value>
 </property>
</configuration>
```

MapReduce 配置文件

配置 mapred-site.xml

```html
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<!-- 指定 MapReduce 程序运行在 Yarn 上 -->
 <property>
 <name>mapreduce.framework.name</name>
 <value>yarn</value>
 </property>
</configuration>
```

 如果在102上配置完了，那要分发到103 104里

xsync hadoop/



## 群起集群

### **1.配置workers**

vim /opt/module/hadoop-3.1.3/etc/hadoop/workers

添加三台服务器的主机名称（ip地址） 不能有空格

hadoop102

hadoop103

hadoop104

同步所有节点配置文件

xsync /opt/module/hadoop-3.1.3/etc



### **2.启动集群**

路径回到hadoop-3.1.3根目录

**在NameNode节点 启动hdfs：**

1）第一次要初始化：**hdfs namenode -format**

注意：格式 化 NameNode，会产生新的集群 id，导致 NameNode 和 DataNode 的集群 id 不一致，集群找不到已往数据。如果集群在运行过程中报错，需要重新格式化 NameNode 的话，一定要先停止 namenode 和 datanode 进程，并且要删除所有机器的 data 和 logs 目录，然后再进行格式化。

此时 data里多了dfs/name/current/*

里面有个VERSION

```
#Sat Jun 25 00:55:31 CST 2022
namespaceID=1843125508
clusterID=CID-ef92c2a6-66af-4d29-9079-750337a0b366
cTime=1656089731381
storageType=NAME_NODE
blockpoolID=BP-1148931872-192.168.10.102-1656089731381
layoutVersion=-64
```

2）启动集群：sbin/start-dfs.sh

然后去102 103 104 用jps命令 看看是否有正确启动



3）查看后台

namenode ip地址 9870端口

查看 HDFS 上存储的数据信息：Utilities/Browse the file system



**在配置了 ResourceManager 的节点（hadoop103）启动 YARN**

sbin/start-yarn.sh

Web 端查看 YARN 的 ResourceManager：ip地址 8088端口

查看 YARN 上运行的 Job 信息



### 3.集群测试

**上传小文件**

到hadoop目录下 新建input文件夹，并把文件扔进hdfs的input

**hadoop fs -mkdir /input**

**hadoop fs -put  $HADOOP_HOME/wcinput/word.txt /input**



**上传大文件**

**hadoop fs -put /opt/software/jdk-8u212- linux-x64.tar.gz /**



**查看本地文件**

每个集群切开的文件都配置文件里配置了存储路径

data/dfs/data/current/



**下载文件**

 hadoop fs -get /jdk-8u212-linuxx64.tar.gz ./



**执行 wordcount 程序**

测试YARN

hadoop jar  share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar  wordcount /input /output



## 配置历史服务器

为了查看程序的历史运行情况，需要配置一下历史服务器。

**1）配置 mapred-site.xml**

```xml
<!-- 历史服务器端地址 -->
<property>
 <name>mapreduce.jobhistory.address</name>
 <value>hadoop102:10020</value>
</property>
<!-- 历史服务器 web 端地址 -->
<property>
 <name>mapreduce.jobhistory.webapp.address</name>
 <value>hadoop102:19888</value>
</property>
```



**2）分发配置**

xsync  $HADOOP_HOME/etc/hadoop/mapred-site.xml



**3）在 hadoop102 启动历史服务器**

mapred --daemon start historyserver

jps查看是否启动



查看 JobHistory

http://hadoop102:19888/jobhistory



## 配置日志的聚集

日志聚集概念：应用运行完成以后，将程序运行日志信息上传到 HDFS 系统上。

将每个节点的日记都聚集到hdfs里

日志聚集功能好处：可以方便的查看到程序运行详情，方便开发调试。 注意：开启日志聚集功能，需要重新启动 NodeManager 、ResourceManager 和 HistoryServer。



**1）配置 yarn-site.xml**

```xaml
<!-- 开启日志聚集功能 -->
<property>
 <name>yarn.log-aggregation-enable</name>
 <value>true</value>
</property>
<!-- 设置日志聚集服务器地址 -->
<property> 
 <name>yarn.log.server.url</name> 
 <value>http://hadoop102:19888/jobhistory/logs</value>
</property>
<!-- 设置日志保留时间为 7 天 -->
<property>
 <name>yarn.log-aggregation.retain-seconds</name>
 <value>604800</value>
</property>
```



**2）分发配置**

**3）关闭 NodeManager 、ResourceManager 和 HistoryServer**

sbin/stop-yarn.sh

mapred --daemon stop hostoryserver



**4）启动 NodeManager 、ResourceManage 和 HistoryServer**

start-yarn.sh

mapred --daemon start hostoryserver



**5）执行 WordCount 程序**

hadoop jar  share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar  wordcount /input /output2



**6）查看日志**

（1）历史服务器地址 http://hadoop102:19888/jobhistory

（2）点击jobid下

（3）进入页面后点击logs可查看详情日志



## 集群启停方式

### 常用命令

**1）各个模块分开启动/停止（配置 ssh 是前提）常用**

（1）整体启动/停止 HDFS start-dfs.sh/stop-dfs.sh 

（2）整体启动/停止 YARN start-yarn.sh/stop-yarn.sh



**2）各个服务组件逐一启动/停止** 

（1）分别启动/停止 HDFS 组件 hdfs --daemon start/stop namenode/datanode/secondarynamenode 

（2）启动/停止 YARN yarn --daemon start/stop resourcemanager/nodemanager



### 集群常用脚本

**1）Hadoop 集群启停脚本（包含 HDFS，Yarn，Historyserver）：myhadoop.sh**

cd /home/atguigu/bin 

vim myhadoop.sh

```bash
#!/bin/bash
if [ $# -lt 1 ]
then
 echo "No Args Input..."
 exit ;
fi
case $1 in
"start")
 echo " =================== 启动 hadoop 集群 ==================="
 echo " --------------- 启动 hdfs ---------------"
 ssh hadoop102 "/opt/module/hadoop-3.1.3/sbin/start-dfs.sh"
 echo " --------------- 启动 yarn ---------------"
 sh hadoop103 "/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"
 echo " --------------- 启动 historyserver ---------------"
 ssh hadoop102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon start 
historyserver"
;;
"stop")
 echo " =================== 关闭 hadoop 集群 ==================="
 echo " --------------- 关闭 historyserver ---------------"
 ssh hadoop102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon stop 
historyserver"
 echo " --------------- 关闭 yarn ---------------"
 ssh hadoop103 "/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"
 echo " --------------- 关闭 hdfs ---------------"
 ssh hadoop102 "/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh"
;;
*)
 echo "Input Args Error..."
;;
esac
```

保存后退出，然后赋予脚本执行权限

chmod +x myhadoop.sh



**2）查看三台服务器 Java 进程脚本：jpsall**

```bash
#!/bin/bash
for host in hadoop102 hadoop103 hadoop104
do
 echo =============== $host ===============
 ssh $host jps 
done
```



分发/home/atguigu/bin 目录，保证自定义脚本在三台机器上都可以使用



## 集群时间同步

生产环境：如果服务器能连接外网，不需要时间同步

连接不了外网就要配置时间同步服务

这里设置了103 和 104 与 102产生同步



### 时间服务器配置

（必须root用户）

**查看所有节点 ntpd 服务状态和开机自启动状态**

status ntpd

start ntpd

is-enabled ntpd



**修改 hadoop102 的 ntp.conf 配置文件**

 sudo vim /etc/ntp.conf 

（a）修改 1（授权 192.168.10.0-192.168.10.255 网段上的所有机器可以从这台机器上查 询和同步时间）

\#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap 

为 restrict 192.168.10.0 mask 255.255.255.0 nomodify notrap

（b）修改 2（集群在局域网中，不使用其他互联网上的时间）

server 0.centos.pool.ntp.org iburst 

server 1.centos.pool.ntp.org iburst 

server 2.centos.pool.ntp.org iburst 

server 3.centos.pool.ntp.org iburst

为

#server 0.centos.pool.ntp.org iburst 

#server 1.centos.pool.ntp.org iburst 

#server 2.centos.pool.ntp.org iburst 

#server 3.centos.pool.ntp.org iburst



（c）添加 3（当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中 的其他节点提供时间同步）

server 127.127.1.0 fudge 127.127.1.0 stratum 10



**修改 hadoop102 的/etc/sysconfig/ntpd 文件**

[atguigu@hadoop102 ~]$ sudo vim /etc/sysconfig/ntpd

增加内容如下（让硬件时间与系统时间一起同步） SYNC_HWCLOCK=yes



**重新启动 ntpd 服务**

sudo systemctl start ntpd



**设置 ntpd 服务开机启动**

sudo systemctl enable ntpd



### 其他机器配置

（必须 root 用户）



**关闭所有节点上 ntp 服务和自启动**

sudo systemctl stop ntpd

sudo systemctl disable ntpd

sudo systemctl stop ntpd

sudo systemctl disable ntpd



**在其他机器配置 1 分钟与时间服务器同步一次**

 sudo crontab -e

编写定时任务如下： 

*/1 * * * * /usr/sbin/ntpdate hadoop102



**修改任意机器时间**

sudo date -s "2021-9-11 11:11:11"



**1 分钟后查看机器是否与时间服务器同步**

sudo date

