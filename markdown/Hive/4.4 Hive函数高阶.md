# UDTF - explode函数

explode接收map、array类型的数据作为输入，然后把输入数据中的每个元素拆开变成一行数据，一个元素一行。explode执行效果正好满足于输入一行输出多行，所有叫做UDTF函数。

![image-20220815115852203](picture/image-20220815115852203.png)

一般情况下，explode函数可以直接单独使用即可；

也可以根据业务需要结合lateral view侧视图一起使用。

explode(array) 将array里的每个元素生成一行；explode(map)  

将map里的每一对元素作为一行，其中key为一列，value为一列；

![image-20220815120353997](picture/image-20220815120353997.png)



# 案例

## 题目

![image-20220815120428996](picture/image-20220815120428996.png)

## 建表

```Hive
--step1:建表
create table the_nba_championship(
     team_name string,
     champion_year array<string>
) row format delimited
fields terminated by ','
collection items terminated by '|';

--step2:加载数据文件到表中
load data inpath '/hivedata/The_NBA_Championship.txt' into table the_nba_championship;

--step3:验证
select *
from the_nba_championship;
```

![image-20220815134526238](picture/image-20220815134526238.png)



## 炸开

注意

```
select team_name,explode(champion_year) from the_nba_championship;
```

explode函数属于UDTF表生成函数，explode执行返回的结果可以理解为一张虚拟的表，其数据来源于源表；

在select中只查询源表数据没有问题，只查询explode生成的虚拟表数据也没问题，但是不能在只查询源表的时候，既想返回源表字段又想返回explode生成的虚拟表字段；

通俗点讲，有两张表，不能只查询一张表但是又想返回分别属于两张表的字段；



**解决方案**

从SQL层面上来说上述问题的解决方案是：对两张表进行join关联查询;Hive专门提供了语法lateral View侧视图，专门用于搭配explode这样的UDTF函数，以满足上述需要。

```hive
select a.team_name,b.year
from the_nba_championship a
lateral view explode(champion_year) b as year
order by b.year desc;
```

![image-20220815135507964](picture/image-20220815135507964.png)



# Lateral View 侧视图

Lateral View是一种特殊的语法，主要搭配UDTF类型函数一起使用，用于解决UDTF函数的一些查询限制的问题。一般只要使用UDTF，就会固定搭配lateral view使用。

官方链接：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LateralView

![image-20220815135702333](picture/image-20220815135702333.png)

## 案例

```HIVE
select *
from the_nba_championship a  --真实的表
lateral view explode(champion_year) b as year  --UDTF表 别名 as 字段
```

![image-20220815140613472](picture/image-20220815140613472.png)





## 统计次数

```hive
select a.team_name, count(*) as nums
from the_nba_championship a  --真实的表
lateral view explode(champion_year) b as year  --UDTF表 别名 as 字段
group by team_name
order by nums desc;
```



<img src="picture/image-20220815140935275.png" alt="image-20220815140935275" style="zoom:50%;" />



# Arregation 聚合函数

聚合函数的功能是：对一组值执行计算并返回单一的值。

聚合函数是典型的输入多行输出一行，使用Hive的分类标准，属于UDAF类型函数。通常搭配Group By语法一起使用，分组后进行聚合操作。

![image-20220815141101129](picture/image-20220815141101129.png)

## 基础聚合

HQL提供了几种内置的UDAF聚合函数，例如max（...），min（...）和avg（...）。这些我们把它称之为基础的聚合函数。通常情况下聚合函数会与GROUP BY子句一起使用。如果未指定GROUP BY子句，默认情况下，它会汇总所有行数据。



数据准备

```Hive
--1、测试数据准备
drop table if exists student;
create table student(
    num int,
    name string,
    sex string,
    age int,
    dept string)
    row format delimited
fields terminated by ',';
--加载数据
load data inpath '/hivedata/students.txt' into table student;
--验证
select * from student;
```

![image-20220815141448205](picture/image-20220815141448205.png)



```hive
--场景1：没有group by子句的聚合操作
--count(*)：所有行进行统计，包括NULL行
--count(1)：所有行进行统计，包括NULL行
--count(column)：对column中非Null进行统计
select count(*) as cnt1,count(1) as cnt2 from student;
```



```Hive
--场景2：带有group by子句的聚合操作 注意group by语法限制
select sex,count(*) as cnt from student group by sex;
```

<img src="picture/image-20220815141722547.png" alt="image-20220815141722547" style="zoom:50%;" />

```hive
--场景3：select时多个聚合函数一起使用
select count(*) as cnt1,avg(age) as cnt2 from student;
```

![image-20220815141835816](picture/image-20220815141835816.png)



```hive
--场景4：聚合函数和case when条件转换函数、coalesce函数、if函数使用
select
    sum(CASE WHEN sex = '男'THEN 1 ELSE 0 END)
from student;

select
    sum(if(sex = '男',1,0))
from student;

--场景5：聚合参数不支持嵌套聚合函数
select avg(count(*))  from student;
```

```hive
--场景6：聚合操作时针对null的处理
CREATE TABLE tmp_1 (val1 int, val2 int);
INSERT INTO TABLE tmp_1 VALUES (1, 2),(null,2),(2,3);
select * from tmp_1;
```

![image-20220815142116162](picture/image-20220815142116162.png)

```hive
--第二行数据(NULL, 2) 在进行sum(val1 + val2)的时候会被忽略
select sum(val1), sum(val1 + val2) from tmp_1;
```

![image-20220815142210616](picture/image-20220815142210616.png)

```hive
--可以使用coalesce函数解决
select
    sum(coalesce(val1,0)) -- 返回第一个不为空的值，如果没有就返回0
    sum(coalesce(val1,0) + val2)
from tmp_1;
```

<img src="picture/image-20220815142458495.png" alt="image-20220815142458495" style="zoom:50%;" />

```hive
--场景7：配合distinct关键字去重聚合
--此场景下，会编译期间会自动设置只启动一个reduce task处理数据  可能造成数据拥堵
select count(distinct sex) as cnt1 from student;
--可以先去重 在聚合 通过子查询完成
--因为先执行distinct的时候 可以使用多个reducetask来跑数据
select count(*) as gender_uni_cnt
from (select distinct sex from student) a;
```



```hive
--案例需求：找出student中男女学生年龄最大的及其名字
--这里使用了struct来构造数据 然后针对struct应用max找出最大元素 然后取值
select sex,
max(struct(age, name)).col1 as age,
max(struct(age, name)).col2 as name
from student
group by sex;

select struct(age, name) from student;
select struct(age, name).col1 from student;
select max(struct(age, name)) from student;
```



## 增强聚合

增强聚合包括grouping_sets、cube、rollup这几个函数；主要适用于OLAP多维数据分析模式中，多维分析中的维指的分析问题时看待问题的维度、角度。下面通过案例更好的理解函数的功能含义。数据中字段含义：月份、天、用户标识cookieid。

![image-20220815150038260](picture/image-20220815150038260.png)



## grouping sets

grouping sets是一种将多个group by逻辑写在一个sql语句中的便利写法。

等价于将不同维度的GROUP BY结果集进行UNION ALL。GROUPING__ID表示结果属于哪一个分组集合。



# Window function 窗口函数

![image-20220815150956615](picture/image-20220815150956615.png)



## 窗口聚合函数

### 加载数据

```hive
--建表加载数据
CREATE TABLE employee(
     id int,
     name string,
     deg string,
     salary int,
     dept string
) row format delimited
    fields terminated by ',';

load data inpath '/hivedata/employee.txt' into table employee;

select * from employee;
```

![image-20220815151327612](picture/image-20220815151327612.png)

![image-20220815151533935](picture/image-20220815151533935.png)

中间细节隐藏掉了

### 窗口函数

![image-20220815151702173](picture/image-20220815151702173.png)



### 语法

![image-20220815151741888](picture/image-20220815151741888.png)

### 案例

<img src="picture/image-20220815152023228.png" alt="image-20220815152023228" style="zoom:50%;" />

<img src="picture/image-20220815152039602.png" alt="image-20220815152039602" style="zoom:50%;" />



**普通聚合**

![image-20220815152228385](picture/image-20220815152228385.png)



**窗口函数聚合**

1.所有表求和

![image-20220815152413624](picture/image-20220815152413624.png)

2.组内求和 partition by

![image-20220815152512298](picture/image-20220815152512298.png)

3.组内累计求和 order by

![image-20220815152705779](picture/image-20220815152705779.png)

![image-20220815152803721](picture/image-20220815152803721.png)



## 窗口表达式-规则求和

![image-20220815152910014](picture/image-20220815152910014.png)

**第一行到当前行**

![image-20220815153034534](picture/image-20220815153034534.png)



**向前3行到当前行**

![image-20220815153318127](picture/image-20220815153318127.png)

```hive
--向前3行 向后1行
select cookieid,createtime,pv,
       sum(pv) over(partition by cookieid order by createtime rows between 3 preceding and 1 following) as pv5
from website_pv_info;

--当前行至最后一行
select cookieid,createtime,pv,
       sum(pv) over(partition by cookieid order by createtime rows between current row and unbounded following) as pv6
from website_pv_info;

--第一行到最后一行 也就是分组内的所有行
select cookieid,createtime,pv,
       sum(pv) over(partition by cookieid order by createtime rows between unbounded preceding  and unbounded following) as pv6
from website_pv_info;
```



## 窗口排序函数-打序号

![image-20220815153441437](picture/image-20220815153441437.png)

![image-20220815153926627](picture/image-20220815153926627.png)

**非常适合TopN业务分析**



需求：找出每个用户访问pv最多的Top3 重复并列的不考虑

![image-20220815154152764](picture/image-20220815154152764.png)



## 窗口排序函数-ntile

![image-20220815154417725](picture/image-20220815154417725.png)

```Hive
--需求：统计每个用户pv数最多的前3分之1天。
--理解：将数据根据cookieid分 根据pv倒序排序 排序之后分为3个部分 取第一部分
SELECT * from
    (SELECT
         cookieid,
         createtime,
         pv,
         NTILE(3) OVER(PARTITION BY cookieid ORDER BY pv DESC) AS rn
     FROM website_pv_info) tmp where rn =1;
select * from website_url_info;
```



## 窗口分析函数

![image-20220815154653104](picture/image-20220815154653104.png)

```Hive
-----------窗口分析函数----------
--LAG
SELECT cookieid,
       createtime,
       url,
       ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY createtime) AS rn,
        LAG(createtime,1,'1970-01-01 00:00:00') OVER(PARTITION BY cookieid ORDER BY createtime) AS last_1_time,
        LAG(createtime,2) OVER(PARTITION BY cookieid ORDER BY createtime) AS last_2_time
FROM website_url_info;
```

![image-20220815154948437](picture/image-20220815154948437.png)



```hive
--LEAD
SELECT cookieid,
       createtime,
       url,
       ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY createtime) AS rn,
        LEAD(createtime,1,'1970-01-01 00:00:00') OVER(PARTITION BY cookieid ORDER BY createtime) AS next_1_time,
        LEAD(createtime,2) OVER(PARTITION BY cookieid ORDER BY createtime) AS next_2_time
FROM website_url_info;

--FIRST_VALUE
SELECT cookieid,
       createtime,
       url,
       ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY createtime) AS rn,
        FIRST_VALUE(url) OVER(PARTITION BY cookieid ORDER BY createtime) AS first1
FROM website_url_info;

--LAST_VALUE
SELECT cookieid,
       createtime,
       url,
       ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY createtime) AS rn,
        LAST_VALUE(url) OVER(PARTITION BY cookieid ORDER BY createtime) AS last1
FROM website_url_info;
```



# Sampling 抽样函数

当数据量过大时，我们可能需要查找数据子集加快数据处理速度分析

这就是抽样、采样，一种用于识别 和 分析数据中的子集的技术，以发现整个数据集中的模式和趋势

在HQL中，可以通过三种方式采样数据：随机采样。存储桶表采样 和 块采样

![image-20220816155426769](picture/image-20220816155426769.png)

## Random随机抽样

随机抽样使用rand（）函数来确保随机获取数据，LIMIT来限制抽取的数据个数。 

优点是随机，缺点是速度不快，尤其表数据多的时候。推荐DISTRIBUTE+SORT，可以确保数据也随机分布在mapper和reducer之间，使得底层执行有效率。 

ORDER BY语句也可以达到相同的目的，但是表现不好，因为ORDER BY是全局排序，只会启动运行一个reducer 

```hive
--数据表
select * from student;

--需求：随机抽取2个学生的情况进行查看
SELECT * FROM student
DISTRIBUTE BY rand() SORT BY rand() LIMIT 2;

--使用order by+rand也可以实现同样的效果 但是效率不高
SELECT * FROM student
ORDER BY rand() LIMIT 2;
```



## Block基于数据块抽样

Block块采样允许随机获取n行数据、百分比数据或指定大小的数据。 

采样粒度是HDFS块大小。

优点是速度快，缺点是不随机。

```hive
---block抽样
--根据行数抽样
SELECT * FROM student TABLESAMPLE(1 ROWS);

--根据数据大小百分比抽样
SELECT * FROM student TABLESAMPLE(50 PERCENT);

--根据数据大小抽样
--支持数据单位 b/B, k/K, m/M, g/G
SELECT * FROM student TABLESAMPLE(1k);

```



## Bucket table 基于分桶表抽样

这是一种特殊的采样方法，针对分桶表进行了优化。优点是既随机速度也很快。语法如下：

![image-20220816155859331](picture/image-20220816155859331.png)

--TABLESAMPLE (BUCKET x OUT OF y [ON colname])

1、y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。
例如，table总共分了4份（4个bucket），当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据。

2、x表示从哪个bucket开始抽取。
例如，table总bucket数为4，tablesample(bucket 4 out of 4)，表示总共抽取（4/4=）1个bucket的数据，抽取第4个bucket的数据。
注意：x的值必须小于等于y的值，否则FAILED:Numerator should not be bigger than denominator in sample clause for table stu_buck

3、ON colname表示基于什么抽
ON rand()表示随机抽
ON 分桶字段 表示基于分桶字段抽样 效率更高 推荐

```hive
---bucket table抽样
--根据整行数据进行抽样
SELECT * FROM t_usa_covid19_bucket TABLESAMPLE(BUCKET 1 OUT OF 2 ON rand());

--根据分桶字段进行抽样 效率更高
describe formatted t_usa_covid19_bucket;
SELECT * FROM t_usa_covid19_bucket TABLESAMPLE(BUCKET 1 OUT OF 2 ON state);

```

